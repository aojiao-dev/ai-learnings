# ðŸš€ ML & LLM Study Plan

[![Python](https://img.shields.io/badge/Python-3.12-blue.svg)]()
[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?logo=pytorch&logoColor=white)]()
[![TensorFlow](https://img.shields.io/badge/TensorFlow-%23FF6F00.svg?logo=tensorflow&logoColor=white)]()
[![License](https://img.shields.io/badge/license-MIT-green.svg)]()

This repository documents my personal learning journey as I transition from a **traditional search engineer** to an **AI/ML engineer**.  
It includes study notes, code snippets, and mini-projects that build toward developing and fine-tuning domain-specific LLMs.

---

## ðŸ§© Learning Milestones

### 1. Review ML Fundamentals
Andrew Ngâ€™s *Machine Learning Specialization* is my starting point.  
It covers essential concepts like:
- Linear & logistic regression  
- Loss functions and gradient descent  
- Supervised vs. unsupervised learning  

It also introduces more advanced topics such as **neural networks**, **decision trees**, **clustering**, **recommender systems**, and **reinforcement learning**.

---

### 2. Review DL Fundamentals
Andrew Ngâ€™s *Deep Learning Specialization* focuses on:
- **Recurrent Neural Networks (RNNs)**
- **Convolutional Neural Networks (CNNs)**
- **Sequence models and Transformers** (which modern LLMs are based on)

---

### 3. Cross-check Understanding
To reinforce what Iâ€™ve learned, Iâ€™ll watch and follow along with other experts who explain neural networks and LLMs in concise, intuitive ways.  
If a topic doesnâ€™t make sense, it means I need to revisit the fundamentals.

**Resources:**
- ðŸ§  3Blue1Brown â€“ [Deep Learning](https://www.youtube.com/watch?v=aircAruvnKk)  
- ðŸ’» Andrej Karpathy â€“ [Zero to Hero](https://karpathy.ai/zero-to-hero.html)  
- ðŸ§© OpenAI Blog â€“ [Spinning Up](https://spinningup.openai.com/en/latest/user/introduction.html)  
- ðŸ“˜ Brandon Rohrer â€“ [Transformers from Scratch](https://brandonrohrer.com/transformers.html)

---

### 4. Hands-on Coding
The goal is to **build from scratch** for deeper intuition â€” not just use pre-built frameworks.

#### Milestones:
- **NN from Scratch:** Build simple MLP, RNN, and CNN models for image recognition and sequence generation.  
- **LLM from Scratch:** Build a minimal transformer-based model and pre-train it on small text datasets (e.g., Common Crawl subsets).  
  > Inspired by Stanfordâ€™s CS336, but without going all the way down to tokenizer training.
- **LLM Post-training:**  
  - Fine-tune an open-source GPT-OSS model via **SFT** to act as a shopping Q&A agent with tool-use abilities.  
  - Apply **RLHF** (via DPO or PPO) to align the modelâ€™s tone and helpfulness to user preferences.  
  - Build an **E2E demo**: GPT-4 handles query classification and routes shopping-related questions to the fine-tuned model.

---

## ðŸ§° Useful Commands

```bash
uv init
uv add tensorflow
uv add torch
uv add torchvision
uv add matplotlib
